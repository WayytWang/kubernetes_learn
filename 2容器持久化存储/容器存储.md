kubernetes项目处理容器持久化存储的核心原理



Persistent Volume

Persistent Volume Claim



## PV和PVC

PV描述的是持久化存储数据卷。这个API对象主要定义的是一个持久化存储在宿主机上的目录，比如一个NFS的挂载目录。

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
	name: nfs
spec:
	storageClassName: manual
	capacity:
		storage: 1Gi
	accessModes:
    	- ReadWriteMany
    nfs:
    	server: 10.244.1.4
    	path: "/"
```

PVC描述的则是Pod所希望使用的持久化存储的属性。

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
	name: nfs
spec:
	accessModes:
		- ReadWriteMany
	storageClassName: manual
    resources:
    	request:
    		storage: 1Gi
```

用户创建的PVC要真正的被容器使用起来，必须先和某个符合条件的PV进行绑定。PVC和PV绑定的条件包括两部分：

- PV和PVC的spec字段，比如PV的spec.capacity.storage必须满足PVC的spec.resources.request.storage

- PV和PVC的spec.storageClassName字段必须一样。

PVC和PV绑定后，Pod就能够像使用hostPath等常规类型的Volume一样，在YAML文件中声明使用这个PVC了：

```yaml
apiVersion: v1
kind: Pod
metadata:
	labels:
		role: web-fronted
spec:
	containers:
	- name: web
	  image: nginx
	  ports:
	  	- name: web
	  	  containerPort: 80
	  volumeMounts:
      	- name: nfs
      	  mountPath: "/usr/share/nginx/html"
    volumes:
    - name: nfs
      persistentVolumeClaim:
      	claimName: nfs
```

Pod创建之后，kubelet就会把这个PVC所对应的PV，也就是一个NFS类型的Volume挂载到这个Pod容器内的目录上。

当创建Pod时，系统里并没有合适的PV跟它定义的PVC绑定，也就是想要的Volume不存在，这时Pod的启动就会报错。当对应的PV被创建出来之后，kubernetes怎样才能再次完成PVC和PV的绑定操作，从而启动Pod？



## PersistentVolumeController

还是需要控制器，kubernetes中，存在一个专门处理持久化存储的控制器，叫做Volume Controller。它有多个控制循环，其中有一个就是PersistentVolumeController。

它会不断查看当前每一个PVC是不是已处于“Bound”状态，如果不是，就会遍历所有的可用的PV，并尝试绑定。

而这个绑定就是将这个PV对象的名字，填在PVC对象的spec.volumeName字段上。所以kubernetes能通过PVC找到PV。



## PV这个对象，是如何变成容器里的一个持久化存储呢?

容器的Volmue，其实就是将一个宿主机上的目录，跟一个容器里的目录绑定挂载在了一起，而持久化就是指这个宿主机上的目录，具备“持久性”。

- 这个目录里面的内容，既不会因为容器的删除而被清理掉，也不会跟当前的宿主机绑定，这样容器重启或者在其他节点上重建出来之后，它仍然能够通过挂载这个Volume，访问到这些内容。
- hostPath和emptyDir类型的Volume不具备这个特性，它们可能被kubelet清理掉，也不能被“迁移”到其他节点上。
- 持久化Volume的实现，往往依赖于一个远程存储服务，比如：远程文件存储（NFS、GlusterFS）、远程块存储（公有云提供的远程磁盘）等
- kubernetes需要做的的工作，就是使用这些存储服务，来为容器准备一个持久化的宿主机目录，以供将来进行绑定挂载使用。

### 两阶段处理

持久化宿主机目录的过程，可以称为”两阶段处理“

- 当一个Pod调度到一个节点上之后，kubelet就要负责为这个Pod创建它的Volume目录，默认情况下，为：

  ```
  /var/lib/kubelet/pods/<Pod的ID>/volumes/kubernetes.io-<Volume类型>/<Volume名字>
  ```

  - Pod的ID可以通过`kubectl get pods podName -o yaml`，在yaml文件中metadata.uid查看到

- 接下来kubelet要做的操作就取决于Volume类型了

  - 如果Volume类型是远程块存储，比如Google Cloud的Persistent Disk，那么kubelet就需要先调用Google Cloud的API，讲它所提供的Persistent Disk挂载到Pod所在的宿主机上。（块存储就理解成一块磁盘）

  - 相当于执行

    `gcloud compute instances attach-disk <虚拟机名字> --disk <远程磁盘名字>`

- 为虚拟机挂载远程磁盘的操作，就是第一阶段，即**Attach**

- Attach完成后，kubelet还要进行第二个操作：格式化这个磁盘设备，然后将它挂载到宿主机指定的挂载点上，这个挂载点就是Volume的宿主机目录。这个阶段成为**Mount**

- Mount完成后，容器在它里面写入的内容，会保存在Google Cloud的远程磁盘中。

- 对于NFS这种远程文件存储的Volume，kubelet的处理过程就会更简单一些。一般远程文件存储没有一个存储设备要挂载在宿主机上。可以跳过Attach操作。只需要Mount。

- 通过两阶段的处理，就得到了一个持久化的Volume宿主机目录，kubelet只要把这个Volume目录通过CRI里的Mounts参数，传递给Docker，就可以为Pod里的容器挂载这个持久化的Volume了。

- 当删除一个PV时，kubernetes也需要Unmount和Dettach两个阶段来反向处理。

### 两阶段的控制循环

PV的处理流程和Pod以及容器的启动实际没有太多耦合，只要kubelet在向Docker发起CRI请求之前，确保持久化的宿主机目录已经处理完毕即可。

所以u”两阶段处理“的流程时靠独立于kubelet主控制循环(Kubelet Sync Loop)之外的两个控制器来实现的。

- 第一阶段的Attach由AttachDetachController维护。Attach操作只需要调用存储项目的API即可，运行在Master节点即可。
- 第二阶段的Mount由VolumeManagerReconciler维护。Mount操作必须发生在Pod对应的宿主机上，所以它必须是kubelet组件的一部分。它运行起来后，是独立于kubelet主循环的Goroutine。

Volume的处理和kubelet主循环解耦，就避免了Pod的创建效率被拖慢。kubelet的一个主要设计原则，就是它的主控制循环绝对不可以被block。

### 

## StorageClass

- 当PVC很多的时候，也需要创建很多PV，手工操作（Static Provisioning）几乎不可能。

- kubernetes提供了一套可以自动创建PV的机制，即Dynamic Provisioning。

- Dynamic Provisioning的核心在于StorageClass。StorageClass对象的作用其实是创建PV的模板。

- StorageClass会定义如下两个部分的内容

  - PV的属性，比如存储类型，Volume的大小等
  - PV需要用到的存储插件，比如Ceph等

- 这样kubernetes就能根据PVC找到对应的StorageClass，然后就会调用该StorageClass声明的存储插件，创建出需要的PV。

- 假如Volume的类型是GCE的Persistent Disk的话，运维人员就需要定义一个如下的StorageClass：

  ```yaml
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
  	name: block-service
  provisioner: kubernetes.io/gre-pd
  parameters:
  	type: pd-ssd
  ```

  - 定义了一个name：block-service的StorageClass

  - provisioner对应的kubernetes.io/gre-pd是kubernetes内置的GCE PD存储插件的名字

  - parameters是PV的参数，表示这个PV的类型是”SSD格式的GCE远程磁盘“

  - 在PVC中指定要使用的StorageClass名字即可

    ```yaml
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
    	name: claim
    spec:
    	accessModes:
    		- ReadWriteOnce
    	storageClassName: block-service
        resources:
        	requests:
        		storage: 30Gi
    ```

  - 创建PVC时，kubernetes就会绑定一个自动创建的PV。

  - Dynamic Provisioning机制下，只需要在集群中创建数量有限的StorageClass对象就可以了，Kubernetes有默认支持Dynamic Provisioning的存储插件，对于其他非内置存储插件，可以自己编写外部插件来实现。todo：学到这里后再回过头来总结

  - 但是StorageClass并不是专门为了Dynamic Provisioning而设计的

    - 上面用到的”manual“，集群中并没有这个StorageClass，这个时候kubernetes进行的时Static Provisioning，但在做决策时，它依然会考虑PV和PVC的StorageClass定义。todo：没懂
    - 这样的好处就是PV和PVC的定义，完全在自己的掌握中

  - 集群还可以开启名叫DefaultStorageClass的Admission Plugin，它会自动为PVC和PV添加默认的StorageClass，否则PVC的StorageClassName就是”“，也只能和StorageClassName为”“的PV绑定。


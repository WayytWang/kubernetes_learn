---
typora-root-url: pic
---



# 容器网络

## 单机游戏

Linux容器能够看见的“网络栈”，实际上是被隔离在它自己的Network Namespace当中的，一个网络栈包括：

- 网卡 Network Interface

- 回环设备 Loopback Device

- 路由表 Routing Table

- iptables规则

虽然容器可以直接使用主机的网络栈，这样会不可避免地引入共享网络资源地问题，比如端口冲突，所以在大多数情况下，都希望容器能使用独立的Network Namespace，即：拥有属于自己的IP地址和端口。

Linux容器可以看成是一台主机，多台主机之间的网络通信需要网线和交换机。

Linux中起到交换机作用的是网桥 Bridge

- 它工作在数据链路层 Data Link，主要功能是根据MAC地址学习来将数据包转发到网桥的不同端口Port上。

Docker项目会默认在宿主机上创建一个名叫docker0的网桥，凡是连接在docker0网桥上的容器，都可以通过它来通信。

- 连接需要用到Veth Pair的虚拟设备
- Veth Pari被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现的。其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，哪怕这两张网卡在不同的Network Namespace里



### 具体体现

基于nginx镜像，构建了一个有net-tools的镜像，叫nginx-net

首先启动一个容器，并进入容器

```bash
# 启动容器
docker run -d --name nginx-1 nginx-net:v1
# 进入容器
docker exec -it nginx-1 /bin/bash
```

在容器内部使用ifconfig看到网卡设备

 ![](ifconfig-in-container1.png)

能看到有一张eth0的网卡，它正式Veth Pair设备在容器里的这一端，另一端则在宿主机上

 ![](/ifconfig-in-computer.png)

vethf0dc1fe就是与刚刚nginx-1容器eth0网卡组成Veth Pair的虚拟网卡，通过`brctl show`命令可以看到，这张网卡被插在了docker0网桥上。

同样的方式启动另一个容器nginx-2，这时docker0网桥上会多插一张虚拟网卡，对应着nginx-2的eth0。

 ![ifconfig-in-container2](/ifconfig-in-container2.png)

 ![](/brctl-show-container2.png)


两个容器启动好了，对应的网络设备也清晰了，接下来在nginx-1容器中ping一下nginx-2容器

 ![](/c1-ping-c2.png)

可以看到两个容器之间是可以互相网络访问的。

从nginx-1中查看路由规则：

 ![](/route-in-container1.png)

- 从route的第二条规则来看，Destination和Genmask决定了，去向172.17开头的ip地址数据，都会走eth0网卡
- nginx-2的IP地址是172.17.0.3，所以也会走eth0网卡
- eth0的另一端在宿主机的Network Namespace中，并且那一端插在了docker0上
  - 一旦一张虚拟网卡插在了网桥上，那这张网卡就变成了从设备，变成了网桥上的一个端口，只能接收流入的数据
  - nginx-2的eth0也是一样的情况
- 当nginx-1发数据给nginx-2时，需要nginx-2的MAC地址
  - ARP协议
  - nginx-1的eth0发出ARP广播，就是它的另一端发出广播，实际上发出广播的是docker0
  - nginx-2的宿主机的Veth Peer是插在docker0上的，所以相当于nginx-2的eth0收到了广播，从而将MAC地址发出去了
  - nginx-1知道了nginx-2的MAC地址
- nginx-1有了MAC地址后，要发正式的数据了
  - nginx-1的eth0发数据，就相当于docker0发数据
  - docker0拿着nginx-2的MAC地址，根据CAM表（交换机通过MAC地址学习维护的端口和MAC地址的对应表）就知道了nginx-2的Veth Peer，然后发送数据
  - nginx-2的eth0同时也收到了数据
  - 响应数据原路返回

如果是从宿主机访问容器IP时，从宿主机的route可以看到这条规则：

 ![](/route-in-computer.png)

走向容器的网络会走到docker0网卡，然后准发到对应的Veth Peer

如果是从容器试图连接另外一个宿主机时

- 请求数据包先到通docker0网桥出现到了宿主机
- 宿主机向外部ip发出请求会走eth0（default的路由规则）
- 宿主机eth0将数据发到另一个宿主机

当一台宿主机的容器往另一台宿主机的容器发送请求时呢？

- 源端容器和目的端容器的IP都是各自docker0上的
- 需要更大的网桥，Overlay Network

todo：学习iptables



## 容器跨主机网络

Flannel的三种后端实现

- VXLAN
- host-gw
- UDP

UDP模式

跨主机的容器通信

flannel0设备的类型：TUN设备 Tunnel

- 在操作系统内核和用户应用程序之间传递IP包
- 操作系统将IP包发送个flannel0后，flannel会将IP包交给Flannel进程 （内核态 --> 用户态）
- Flannel进程拿到IP包后，就会发哦是那个给Node2主机

怎么找到知道容器Node2

- 子网 subnet
- 宿主机上的所有容器都属于该宿主机被分配的一个子网
- 子网 与 宿主机的IP 存在etcd中

Node2怎么处理

- flanneld收到UDP包，解开拿到原来的IP包
- flannel0设备将IP包放入操作系统（用户态 --> 内核态）
- flannel0 -> docker0 -> 容器内

性能不好

- 容器发出的IP包，经过docker0进入内核态
- IP包进入TUN设备，到用户态
- flanneld 进行UDP封装后，重新进入内核态
  - 这个封装过程能在内核态吗？

VXLAN 

- Virtual Extensible LAN 虚拟可扩展局域网
- 完全在内核态实现封装和解封的工作
- VTEP VXLAN Tunnel End Point flannel.1

原始IP包到了flannel.1后，找出口，通过路由规则可以知道 出口就是目标VTEP设备

目的VTEP设别的MAC地址是什么。这里也是通过路由规则找到的

用UDP包装这个内部数据帧 + VNI标志

另一端flannel.1设备的IP地址是什么

FDB的转发数据库



# 同一宿主机上的容器间访问：

- docker项目会创建一个docker0网桥
  - 网桥是一个二层交换机，交换机上的端口插着网卡
  - 网桥的主要是作用是转发，它会把数据包的mac头取消来，检查目的mac地址，然后找到mac地址对应的网卡的端口，将数据包转发过去

- 容器创建时，宿主机上创建了一张虚拟网卡，虚拟网卡有两个特点：
  - 插在docker0网桥
  - 虚拟网卡是一个Veth Peer，另一端在对应的容器里面eth0，两端的网卡组成了Veth Pair
- Veth Pair的两端，其中一端网卡发出来的数据包，可以直接出现在另一端网卡上，即使在不同的Network NameSpace中。插在docker0上的一端，就只能收包，然后将包全部交给docker0。不能转发了。
- c1给c2发包：
  - c1构建网络包，到了IP层，根据路由规则（route命令），发现去c2要走eth0网卡
  - eth0在MAC层，网卡发现c2和自己属于同一网段（都连着docker0网桥），于是发起ARP协议获取c2的mac地址
    - eth0构建好arp包后，准备发送时，根据Veth Pair的特性，宿主机的eth0的另一头vethxxxx上也会有arp包
    - vethxxxx只会把包给docker0，docker0广播后，获得c2的mac地址
  - eth0构建好MAC层的包，开始发送
  - 实际上是vethxxxx交给docker0网桥，后发送到c2



# 宿主机访问容器

- 宿主机构建IP层包时，发现去c3要走docker0网桥
- docker0网桥获取对方MAC地址后，转发到c3对应的网卡vethxxxx，发送
- c3的eth0会收到



# 容器访问其他宿主机

- c4构建IP包完后，走c4的eth0，其实就是vethxxxx收到包，转到了docker0
- docker0把这个IP包发到宿主机的eth0网卡
- 然后就是正的机器间通信



# 跨主机的容器通信

## UDP模式

前提条件时，建立了flannel

Node1 c1 访问 Node2 c2

- c1往外的IP包到了docker0网桥
- docker0网桥从IP头根据路由规则，选中flannel0设备
  - TUN设备 Tunnel
  - 工作在第三层，在操作系统和用户应用程序之间传递IP包
- flannel0收到了一个IP包，现在这个IP包的目的IP地址是Node2上面的一个容器IP。需要把这个IP包，包起来用UDP发送给node2先
- flannel0设备会把包给到Flannel进程，由flanneld来发给Node2
  - flanneld怎么知道Node2的IP
  - Flannel管理的容器网络里面，同一宿主机的容器，都属于同一个子网
  - 子网与宿主机的关系存在etcd中
  - 宿主机的IP也存在etcd中
- flanneld目的就是把以上的IP包，用udp去发送，发送到node2中的flanneld
- 往下就是普通的网络通信了

Node2收到udp包后

- flanneld收到了对方的udp包，解出来出来最原始的IP包后，将包给到了flannel0设备
- flannel0设备会根据原始IP包中的ip地址，根据路由规则找到docker0
- docker0会根据转发到对应的容器中去



发送过程中 发生了多少复制

- 容器进程的IP包，到了docker0后，从容器进程用户态进入了内核态
- docker0的IP包，通过TUN进入flanneld 进入了用户态
- flanneld完成udp封装后，进入eth0，又来一次内核态

## VXLAN模式

虚拟可扩展局域网，构建覆盖网络

需要一个VTEP设备 虚拟隧道端点

Node1 的c1 访问 Node2的c2

- c1的IP包到了docker0，docker0交给了flannel1，flannel1要找把包发到哪里去？
- 源VTEP设备和目标VTEP设备之间，需要想办法组成一个虚拟的二层网络，通过二层数据帧进行通信
- 所以首先找到需要目标VTEP(node2)的MAC地址
  - Node2启动时，它的flanneld进程会在Node1上添加自己的ARP记录

- 此时IP包就加上了目标VTEP的MAC地址，但是是不能直接发送的，对方和自己不是一个二层交换机可达的网络
- 这一个构建出来的二层数据帧需要再包装，要包装上Node2的IP和下一跳的MAC地址
- 在封装之前，给内部数据帧加上一个VNI标志
- Linux内核会把这个数据帧封装进一个UDP包里面发出去
  - 宿主机会认为是自己的flannel.1拿到内部数据帧，会认为是和对方的flannel.1发一起一次普通的UDP通信，怎么知道对方flannel.1的信息？
- 内核里面的FDB保存了目标VTEP和它所在主机的IP信息
- Linux内核阶段做完了UDP封装
- Node2收到后，解完IP头后，发现这个包是一个VXLAN，就把内部数据帧交给了flannel.1
- flannel.1解MAC头，解IP头

flannel0设备收到的是一个IP包





# CNI

Kubernetes在启动Infra容器后，就可以直接调用CNI网络插件，为Infra容器的Network Namespace配置符合预期的网络栈

CNI插件时部署k8s时安装好的 /opt/cni/bin

- Main
- IPAM（IP Address Management）
- 社区维护的内置CNI插件

如果要给k8s实现一个可用的容器网络方案，需要做：

- 实现这个网络方案本身
- 实现网络方案对应的cni插件，主要用于配置infra容器里面的网络栈，并把它连接在cni网桥上

然后告诉k8s（通过配置，CNI配置文件），用哪个方案

处理容器网络相关的逻辑不在kublete主干里面，放在了CRI的实现中。

CRI实现,k8s用的 dockershim 会加载上诉的CNI配置文件，dockershim会从配置文件找到该用哪个插件



kubelet创建Pod时，先创建Infra容器

- 这里会调用Docker API创建Infra容器
- 执行SetUpPod方法
  - 为CNI插件准备参数
  - 调用CNI

参数分两部分

- dockershim设置的一组CNI环境变量

  - CNI_COMMAND，是CNI插件唯一需要实现的两个方法

    - ADD（需要的参数）
      - 容器的网卡名字 CNI_IFNAME
      - Pod的Network Namespace文件路径 CNI_NETNS
      - 容器ID CNI_CONTAINERID
    - DEL

- dockershim从CNI配置文件里面加载到的，默认插件的配置信息

CNI不会自己去做，会Delegate指定CNI内置插件完成，这里是CNI bridge插件

bridge：

- 先检查宿主机上的cni网桥是否存在，否则创建
- 进入Infra容器的Network Namespace文件，创建一对Veth Pair
- 另一端移动到宿主机上
- 插到cni网桥上
- 设置hairpin mode
- 调用cni ipam 为容器分配ip，给到容器的eth0
- bridge为cni网桥设置ip地址







# 纯三层的网络方案

## Flannel host-gw

前提条件是机器之间是二层可达的

node1上的c1 访问node2上的c2

- c1的IP包，到了cni网桥
- cni网桥会把ip包给到flannel设备
- flannel设备通过路由规则给到eth0

  - 路由规则：发给xxxx的包 走eth0 下一跳的的ip地址，这ip地址是node2的ip
- 然后查到node2的mac地址，发送到node2
  - 这个很重要，要是不是直接发给node2的，只要到了下一跳，就找不到宿主机了

- node2收到包后，解出ip地址，发现需要交给cni网桥，找到容器

关键是维护好 子网 和 宿主机的关系



## Calico





  









73193891483200





























```
curl --location --request POST 'http://10.99.168.170:8888/api/qyweixin/signature' \
--header 'Content-Type: application/json' \
--data '{
    "url":"https://anyshare.aishu.cn/anyshare/zh-cn/dir/5E9B63FCAF2D440291A55D4DC0887B2C?order_by=time_asc",
    "timestamp":1669278413, 
    "nonceStr": "Wm3WZYTPz0wzccnW"
}'
```



